---
layout: post
title: "《动手学深度学习（第二版）》学习笔记之 1. 引言"
date: 2025-10-29
tags: [AI, notes]
toc: true
comments: true
author: pianfan
---

*机器学习*（machine learning，ML）

*深度学习*（deep learning，DL）<!-- more -->

## 1.1. 日常生活中的机器学习
*数据集*（dataset）

*参数*（parameter）

*模型*（model）：调整参数后的程序。

模型族：通过操作参数而生成的所有不同程序（输入-输出映射）的集合。

*学习算法*（learning algorithm）：使用数据集来选择参数的元程序。

在开始用机器学习算法解决问题之前，我们必须精确地定义问题，确定*输入*（input）和*输出*（output）的性质，并选择合适的模型族。

*学习*（learning）：一个训练模型的过程。通过这个过程，我们可以发现正确的参数集，从而使模型强制执行所需的行为。换句话说，我们用数据*训练*（train）模型。

训练过程通常包含如下步骤：
  1. 从一个随机初始化参数的模型开始，这个模型基本没有“智能”；
  2. 获取一些数据样本；
  3. 调整参数，使模型在这些样本中表现得更好；
  4. 重复第（2）步和第（3）步，直到模型在任务中的表现令人满意。

*用数据编程*（programming with data）：“通过用数据集来确定程序行为”的方法。

## 1.2. 机器学习中的关键组件
核心组件：
  1. 可以用来学习的数据（data）；
  2. 如何转换数据的模型（model）；
  3. 一个目标函数（objective function），用来量化模型的有效性；
  4. 调整模型参数以优化目标函数的算法（algorithm）。

### 1.2.1. 数据
每个数据集由一个个*样本*（example, sample，有时也叫做*数据点*（data point）或者*数据实例*（data instance））组成，大多时候，它们遵循独立同分布(independently and identically distributed, i.i.d.)。

通常每个样本由一组称为*特征*（features，或*协变量*（covariates））的属性组成。

*标签*（label，或*目标*（target））

*维数*（dimensionality）

### 1.2.2. 模型
深度学习与经典方法的区别主要在于：前者关注的功能强大的模型，这些模型由神经网络错综复杂的交织在一起，包含层层数据转换，因此被称为深度学习（deep learning）。

### 1.2.3. 目标函数
*目标函数*（objective function）：模型的优劣程度的度量，大多数情况是“可优化”的。我们通常希望优化它到最低点。 这些函数有时被称为*损失函数*（loss function，或cost function）。 

*平方误差*（squared error）：预测值与实际值之差的平方。试图预测数值时，最常见的损失函数。

错误率：预测与实际情况不符的样本比例。试图解决分类问题时，最常见的目标函数。

有些目标函数（如平方误差）很容易被优化，有些目标（如错误率）由于不可微性或其他复杂性难以直接优化。 在这些情况下，通常会优化*替代目标*。

*训练数据集*（training dataset，或称为*训练集*（training set））：用于拟合模型参数。

*测试数据集*（test dataset，或称为*测试集*（test set））：用于评估拟合的模型。

*过拟合*（overfitting）：一个模型在训练集上表现良好，但不能推广到测试集的现象。

### 1.2.4. 优化算法
优化算法：能够搜索出最佳参数，以最小化损失函数。

*梯度下降*（gradient descent）：在每个步骤中，梯度下降法都会检查每个参数，看看如果仅对该参数进行少量变动，训练集损失会朝哪个方向移动。 然后，它在可以减少损失的方向上优化参数。

## 1.3. 各种机器学习问题

### 1.3.1. 监督学习
*监督学习*（supervised learning）：擅长在“给定输入特征”的情况下预测标签。 每个“特征-标签”对都称为一个*样本*（example）。

监督学习的学习过程一般可以分为三大步骤：
  1. 从已知大量数据样本中随机选取一个子集，为每个样本获取真实标签。这些输入和相应的标签一起构成了训练数据集；
  2. 选择有监督的学习算法，它将训练数据集作为输入，并输出一个“已完成学习的模型”；
  3. 将之前没有见过的样本特征放到这个“已完成学习的模型”中，使用模型的输出作为相应标签的预测。

#### 1.3.1.1. 回归
*回归*（regression）问题：标签取任意数值时的监督学习任务。

#### 1.3.1.2. 分类
*分类*（classification）问题：希望模型能够预测样本属于哪个*类别*（category，正式称为*类*（class））。最简单的分类问题是只有两类，这被称之为*二项分类*（binomial classification）。当有两个以上的类别时，我们把这个问题称为*多项分类*（multiclass classification）问题。

回归是训练一个回归函数来输出一个数值；分类是训练一个分类器来输出预测的类别。

*交叉熵*（cross-entropy）：分类问题的常见损失函数。

*层次分类*（hierarchical classification）：分类任务的变体，可以用于寻找层次结构，层次结构假定在许多类之间存在某种关系。 因此，并不是所有的错误都是均等的。

#### 1.3.1.3. 标记问题
*多标签分类*（multi-label classification）：学习预测不相互排斥的类别的问题。

#### 1.3.1.4. 搜索
有时，我们不仅仅希望输出一个类别或一个实值。 在信息检索领域，我们希望对一组项目进行排序。搜索结果的排序十分重要，学习算法需要输出有序的元素子集。

#### 1.3.1.5. 推荐系统
*推荐系统*（recommender system）：它的目标是向特定用户进行“个性化”推荐。

#### 1.3.1.6. 序列学习
序列学习：需要摄取输入序列或预测输出序列，或两者兼而有之。

### 1.3.2. 无监督学习
*无监督学习*（unsupervised learning）：数据中不含有“目标”的机器学习问题。

- *聚类*（clustering）问题：没有标签的情况下，给数据分类。

- *主成分分析*（principal component analysis）问题：找到少量的参数来准确地捕捉数据的线性相关属性。

- *因果关系*（causality）和*概率图模型*（probabilistic graphical models）问题：描述观察到的许多数据的根本原因。

- *生成对抗性网络*（generative adversarial networks）：提供一种合成数据的方法。

### 1.3.3. 与环境互动
*离线学习*（offline learning）：在算法与环境断开后进行的学习。

*分布偏移*（distribution shift）：环境发生变化的问题。

### 1.3.4. 强化学习
*强化学习*（reinforcement learning）：明确考虑与环境交互的问题。目标是产生一个好的*策略*（policy）。

*深度强化学习*（deep reinforcement learning）：将深度学习应用于强化学习的问题。

智能体（agent）：在一系列的时间步骤上与环境交互。

在每个特定时间点，智能体从环境接收一些*观察*（observation），并且必须选择一个*动作*（action），然后通过某种机制（有时称为执行器）将其传输回环境，最后智能体从环境中获得*奖励*（reward）。此后新一轮循环开始，智能体接收后续观察，并选择后续操作，依此类推。

*学分分配*（credit assignment）问题：决定哪些行为是值得奖励的，哪些行为是需要惩罚的。

*马尔可夫决策过程*（markov decision process）：当环境可被完全观察到时的强化学习问题。

*上下文赌博机*（contextual bandit problem）：当状态不依赖于之前的操作时的强化学习问题。

*多臂赌博机*（multi-armed bandit problem）：当没有状态，只有一组最初未知回报的可用动作时的强化学习问题。

## 1.4. 起源
*估计*（estimation）

*神经网络*（neural networks）

当今大多数网络中都可以找到的几个关键原则：
  - 线性和非线性处理单元的交替，通常称为*层*（layers）；
  - 使用链式规则（也称为*反向传播*（backpropagation））一次性调整网络中的全部参数。

*核方法*（kernel method）

*决策树*（decision tree）

*图模型*（graph models）

## 1.5. 深度学习的发展
- 新的容量控制方法：如 *dropout*，有助于减轻过拟合的危险。这是通过在整个神经网络中应用噪声注入来实现的，出于训练目的，用随机变量来代替权重。

- 注意力机制：不需要记住整个文本序列，所有需要存储的都是指向翻译过程的中间状态的指针。这大大提高了长序列的准确性，因为模型在开始生成新序列之前不再需要记住整个序列。

- 多阶段设计：允许统计建模者描述用于推理的迭代方法。这些工具允许重复修改深度神经网络的内部状态，从而执行推理链中的后续步骤。

- 生成对抗网络：传统模型中，密度估计和生成模型的统计方法侧重于找到合适的概率分布（通常是近似的）和抽样算法。因此，这些算法在很大程度上受到统计模型固有灵活性的限制。生成式对抗性网络的关键创新是用具有可微参数的任意算法代替采样器。然后对这些数据进行调整，使得鉴别器（实际上是一个双样本测试）不能区分假数据和真实数据。通过使用任意算法生成数据的能力，它为各种技术打开了密度估计的大门。

- 并行和分布式训练算法：设计可伸缩算法的关键挑战之一是深度学习优化的主力——随机梯度下降，它依赖于相对较小的小批量数据来处理。同时，小批量限制了 GPU 的效率。

- 并行计算的能力：如果有大量的（状态、动作、奖励）三元组可用，即只要有可能尝试很多东西来了解它们之间的关系，强化学习就会发挥最好的作用。仿真提供了这样一条途径。

- 深度学习框架：允许轻松建模的第一代框架包括 Caffe、Torch 和 Theano 。到目前为止，它们已经被 TensorFlow（通常通过其高级 API Keras 使用）、CNTK、Caffe 2 和 Apache MXNet 所取代。第三代工具，即用于深度学习的命令式工具，它使用类似于 Python NumPy 的语法来描述模型。这个想法被 PyTorch、MXNet 的 Gluon API 和 Jax 都采纳了。

## 1.6. 深度学习的成功案例

## 1.7. 特点
深度学习是“深度”的，模型学习了许多“层”的转换，每一层提供一个层次的表示。由于*表示学习*（representation learning）目的是寻找表示本身，因此深度学习可以称为“多级表示学习”。

深度学习方法中最显著的共同点是使用端到端训练。也就是说，与其基于单独调整的组件组装系统，不如构建系统，然后联合调整它们的性能。

深度学习的一个关键优势是它不仅取代了传统学习管道末端的浅层模型，而且还取代了劳动密集型的特征工程过程。

## 1.8. 小结
- 机器学习研究计算机系统如何利用经验（通常是数据）来提高特定任务的性能。它结合了统计学、数据挖掘和优化的思想。通常，它是被用作实现人工智能解决方案的一种手段。

- 表示学习作为机器学习的一类，其研究的重点是如何自动找到合适的数据表示方式。深度学习是通过学习多层次的转换来进行的多层次的表示学习。

- 深度学习不仅取代了传统机器学习的浅层模型，而且取代了劳动密集型的特征工程。

- 最近在深度学习方面取得的许多进展，大都是由廉价传感器和互联网规模应用所产生的大量数据，以及（通过GPU）算力的突破来触发的。

- 整个系统优化是获得高性能的关键环节。有效的深度学习框架的开源使得这一点的设计和实现变得非常容易。
